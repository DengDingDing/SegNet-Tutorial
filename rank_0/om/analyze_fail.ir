# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
Labels shape value must be equal to the Features except the last dimension of Features

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore/ops/infer/sparse_softmax_cross_entropy_with_logits.cc:49 SparseSoftmaxCrossEntropyWithLogitsInferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:425~426, 8~46
        if not self.sense_flag:
# 1 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:426, 19~46
            return self._no_sens_impl(*inputs)
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~
# 2 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:446~447, 8~95
        if self.use_graceful_exit:
# 3 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:449~453, 8~45
        if self.return_grad:
# 4 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:443, 15~36
        loss = self.network(*inputs)
               ^~~~~~~~~~~~~~~~~~~~~
# 5 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:125, 15~40
        return self._loss_fn(out, label)
               ^~~~~~~~~~~~~~~~~~~~~~~~~
# 6 In file /Users/lmy/Documents/SegNet-Tutorial/train.py:31, 15~42
        return self.loss_fn(logits, label)
               ^~~~~~~~~~~~~~~~~~~~~~~~~~~
# 7 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:858~862, 8~93
        if self.sparse:
# 8 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:859~861, 12~24
            if self.reduction == 'mean':
            ^
# 9 In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:860, 20~69
                x = self.sparse_softmax_cross_entropy(logits, labels)
                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_26
# Total subgraphs: 0

# Attrs:
training: 1

# Total params: 25
# Params:
%para1_inputs0: <null>
%para2_inputs1: <null>
%para3_conv1_1.weight: <Ref[Tensor[Float32]], (64, 3, 3, 3), ref_key=conv1_1.weight>  :  has_default
%para4_conv1_2.weight: <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=conv1_2.weight>  :  has_default
%para5_conv2_1.weight: <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=conv2_1.weight>  :  has_default
%para6_conv2_2.weight: <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=conv2_2.weight>  :  has_default
%para7_conv3_1.weight: <Ref[Tensor[Float32]], (256, 128, 3, 3), ref_key=conv3_1.weight>  :  has_default
%para8_conv3_2.weight: <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=conv3_2.weight>  :  has_default
%para9_conv4_1.weight: <Ref[Tensor[Float32]], (512, 256, 3, 3), ref_key=conv4_1.weight>  :  has_default
%para10_conv4_2.weight: <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=conv4_2.weight>  :  has_default
%para11_conv5_1.weight: <Ref[Tensor[Float32]], (1024, 512, 3, 3), ref_key=conv5_1.weight>  :  has_default
%para12_conv5_2.weight: <Ref[Tensor[Float32]], (1024, 1024, 3, 3), ref_key=conv5_2.weight>  :  has_default
%para13_up_conv_1.weight: <Ref[Tensor[Float32]], (1024, 512, 2, 2), ref_key=up_conv_1.weight>  :  has_default
%para14_conv6_1.weight: <Ref[Tensor[Float32]], (512, 1024, 3, 3), ref_key=conv6_1.weight>  :  has_default
%para15_conv6_2.weight: <Ref[Tensor[Float32]], (512, 512, 3, 3), ref_key=conv6_2.weight>  :  has_default
%para16_up_conv_2.weight: <Ref[Tensor[Float32]], (512, 256, 2, 2), ref_key=up_conv_2.weight>  :  has_default
%para17_conv7_1.weight: <Ref[Tensor[Float32]], (256, 512, 3, 3), ref_key=conv7_1.weight>  :  has_default
%para18_conv7_2.weight: <Ref[Tensor[Float32]], (256, 256, 3, 3), ref_key=conv7_2.weight>  :  has_default
%para19_up_conv_3.weight: <Ref[Tensor[Float32]], (256, 128, 2, 2), ref_key=up_conv_3.weight>  :  has_default
%para20_conv8_1.weight: <Ref[Tensor[Float32]], (128, 256, 3, 3), ref_key=conv8_1.weight>  :  has_default
%para21_conv8_2.weight: <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=conv8_2.weight>  :  has_default
%para22_up_conv_4.weight: <Ref[Tensor[Float32]], (128, 64, 2, 2), ref_key=up_conv_4.weight>  :  has_default
%para23_conv9_1.weight: <Ref[Tensor[Float32]], (64, 128, 3, 3), ref_key=conv9_1.weight>  :  has_default
%para24_conv9_2.weight: <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=conv9_2.weight>  :  has_default
%para25_conv_10.weight: <Ref[Tensor[Float32]], (11, 64, 1, 1), ref_key=conv_10.weight>  :  has_default

subgraph attr:
training: 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_26 : 0x131c54218
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:424~439, 4~19/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_26() {
  %0(CNode_48) = resolve(NameSpace[Entry: 'mindspore.nn.wrap.cell_wrapper.TrainOneStepCell.construct'], mindspore.nn.wrap.cell_wrapper.TrainOneStepCell.construct)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
  %1(CNode_49) = MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (4, 3, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>)
      #scope: (Default)

#------------------------> 0
  %2(CNode_50) = DoUnpackCall(%0, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>) -> (<null>)
      #scope: (Default)
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:425~426, 8~46/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_26:CNode_48{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Entry: 'mindspore.nn.wrap.cell_wrapper.TrainOneStepCell.construct', [2]: ValueNode<Symbol> mindspore.nn.wrap.cell_wrapper.TrainOneStepCell.construct}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_26:CNode_50{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_48, [2]: CNode_49}
#   3: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_26:CNode_51{[0]: ValueNode<Primitive> Return, [1]: CNode_50}


subgraph attr:
core: 1
subgraph instance: UnpackCall_27 : 0x136a86418

subgraph @UnpackCall_27(%para0_Parameter_28, %para0_Parameter_29) {
  %0(CNode_52) = TupleGetItem(%para0_Parameter_29, I64(0))
      : (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 3, 360, 480)>)
      #scope: (Default)
  %1(CNode_53) = TupleGetItem(%para0_Parameter_29, I64(1))
      : (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 1, 360, 480)>)
      #scope: (Default)

#------------------------> 1
  %2(CNode_54) = Parameter_28(%0, %1)
      : (<Tensor[Float32], (4, 3, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<null>)
      #scope: (Default)
  Return(%2)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @UnpackCall_27:CNode_54{[0]: param_Parameter_28, [1]: CNode_52, [2]: CNode_53}
#   2: @UnpackCall_27:CNode_55{[0]: ValueNode<Primitive> Return, [1]: CNode_54}


subgraph attr:
training: 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_30 : 0x131cc0a18
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:424~439, 4~19/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_30(%para0_inputs0, %para0_inputs1) {

#------------------------> 2
  %0(CNode_56) = call @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_31()
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:425~426, 8~46/        if not self.sense_flag:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:425~426, 8~46/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_30:CNode_56{[0]: ValueNode<FuncGraph> ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_31}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_30:CNode_57{[0]: ValueNode<Primitive> Return, [1]: CNode_56}
#   3: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_30:CNode_58{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> _no_sens_impl}


subgraph attr:
training: 1
subgraph instance: ✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_31 : 0x131cc3218
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:424~439, 4~19/    def construct(self, *inputs):/
subgraph @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_31 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_30]() {
  %0(CNode_58) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], _no_sens_impl)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:426, 19~37/            return self._no_sens_impl(*inputs)/
  %1(CNode_59) = MakeTuple(%para0_inputs0, %para0_inputs1)
      : (<Tensor[Float32], (4, 3, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:424, 25~31/    def construct(self, *inputs):/

#------------------------> 3
  %2(CNode_60) = DoUnpackCall(%0, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:426, 19~46/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:426, 12~46/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_31:CNode_60{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_58, [2]: CNode_59}
#   2: @✓mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_31:CNode_61{[0]: ValueNode<Primitive> Return, [1]: CNode_60}


subgraph attr:
core: 1
subgraph instance: UnpackCall_32 : 0x131ccbc18

subgraph @UnpackCall_32(%para0_Parameter_33, %para0_Parameter_34) {
  %0(CNode_62) = TupleGetItem(%para0_Parameter_34, I64(0))
      : (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 3, 360, 480)>)
      #scope: (Default)
  %1(CNode_63) = TupleGetItem(%para0_Parameter_34, I64(1))
      : (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 1, 360, 480)>)
      #scope: (Default)

#------------------------> 4
  %2(CNode_64) = Parameter_33(%0, %1)
      : (<Tensor[Float32], (4, 3, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<null>)
      #scope: (Default)
  Return(%2)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @UnpackCall_32:CNode_64{[0]: param_Parameter_33, [1]: CNode_62, [2]: CNode_63}
#   2: @UnpackCall_32:CNode_65{[0]: ValueNode<Primitive> Return, [1]: CNode_64}


subgraph attr:
training: 1
subgraph instance: _no_sens_impl_35 : 0x131cc1618
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:441~454, 4~19/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_35(%para0_inputs0, %para0_inputs1) {

#------------------------> 5
  %0(CNode_66) = call @✗_no_sens_impl_36()
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:446~447, 8~95/        if self.use_graceful_exit:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:446~447, 8~95/        if self.use_graceful_exit:/
}
# Order:
#   1: @_no_sens_impl_35:CNode_67{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> network}
#   2: @_no_sens_impl_35:loss{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_67, [2]: CNode_68}
#   3: @_no_sens_impl_35:CNode_69{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> grad_no_sens}
#   4: @_no_sens_impl_35:CNode_70{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> network}
#   5: @_no_sens_impl_35:CNode_71{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> weights}
#   6: @_no_sens_impl_72:CNode_73{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   7: @_no_sens_impl_35:CNode_74{[0]: CNode_69, [1]: CNode_70, [2]: CNode_71}
#   8: @_no_sens_impl_35:grads{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_74, [2]: CNode_68}
#   9: @_no_sens_impl_35:CNode_75{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> grad_reducer}
#  10: @_no_sens_impl_72:CNode_76{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  11: @_no_sens_impl_35:grads{[0]: CNode_75, [1]: grads}
#  12: @_no_sens_impl_35:CNode_66{[0]: ValueNode<FuncGraph> ✗_no_sens_impl_36}
#  13: @_no_sens_impl_35:CNode_77{[0]: ValueNode<Primitive> Return, [1]: CNode_66}
#  14: @_no_sens_impl_35:CNode_78{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.nn.wrap.cell_wrapper', [2]: ValueNode<Symbol> F}


subgraph attr:
training: 1
subgraph instance: ✗_no_sens_impl_36 : 0x131cc8a18
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:441~454, 4~19/    def _no_sens_impl(self, *inputs):/
subgraph @✗_no_sens_impl_36 parent: [subgraph @_no_sens_impl_35]() {

#------------------------> 6
  %0(CNode_79) = call @↓_no_sens_impl_37()
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:446~447, 8~95/        if self.use_graceful_exit:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:446~447, 8~95/        if self.use_graceful_exit:/
}
# Order:
#   1: @✗_no_sens_impl_36:CNode_79{[0]: ValueNode<FuncGraph> ↓_no_sens_impl_37}
#   2: @✗_no_sens_impl_36:CNode_80{[0]: ValueNode<Primitive> Return, [1]: CNode_79}


subgraph attr:
training: 1
subgraph instance: ↓_no_sens_impl_37 : 0x131cc9018
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:441~454, 4~19/    def _no_sens_impl(self, *inputs):/
subgraph @↓_no_sens_impl_37 parent: [subgraph @_no_sens_impl_35]() {

#------------------------> 7
  %0(CNode_81) = call @✗↓_no_sens_impl_38()
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:449~453, 8~45/        if self.return_grad:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:449~453, 8~45/        if self.return_grad:/
}
# Order:
#   1: @↓_no_sens_impl_37:CNode_82{[0]: ValueNode<Primitive> getattr, [1]: CNode_78, [2]: ValueNode<StringImm> depend}
#   2: @↓_no_sens_impl_37:CNode_83{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>', [2]: ValueNode<Symbol> optimizer}
#   3: @↓_no_sens_impl_84:CNode_85{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   4: @↓_no_sens_impl_37:CNode_86{[0]: CNode_83, [1]: grads}
#   5: @↓_no_sens_impl_84:CNode_87{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   6: @↓_no_sens_impl_37:loss{[0]: CNode_82, [1]: loss, [2]: CNode_86}
#   7: @↓_no_sens_impl_37:CNode_81{[0]: ValueNode<FuncGraph> ✗↓_no_sens_impl_38}
#   8: @↓_no_sens_impl_37:CNode_88{[0]: ValueNode<Primitive> Return, [1]: CNode_81}


subgraph attr:
training: 1
subgraph instance: ✗↓_no_sens_impl_38 : 0x131ccca18
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:441~454, 4~19/    def _no_sens_impl(self, *inputs):/
subgraph @✗↓_no_sens_impl_38 parent: [subgraph @↓_no_sens_impl_37]() {

#------------------------> 8
  %0(CNode_89) = call @2↓_no_sens_impl_39()
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:449~453, 8~45/        if self.return_grad:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:449~453, 8~45/        if self.return_grad:/
}
# Order:
#   1: @✗↓_no_sens_impl_38:CNode_89{[0]: ValueNode<FuncGraph> 2↓_no_sens_impl_39}
#   2: @✗↓_no_sens_impl_38:CNode_90{[0]: ValueNode<Primitive> Return, [1]: CNode_89}


subgraph attr:
training: 1
subgraph instance: 2↓_no_sens_impl_39 : 0x131ccd018
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:441~454, 4~19/    def _no_sens_impl(self, *inputs):/
subgraph @2↓_no_sens_impl_39 parent: [subgraph @↓_no_sens_impl_37]() {
  %0(CNode_78) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.nn.wrap.cell_wrapper'], F)
      : (<External, NoShape>, <External, NoShape>) -> (<External, NoShape>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:448, 15~16/        loss = F.depend(loss, self.optimizer(grads))/
  %1(CNode_82) = getattr(%0, "depend")
      : (<External, NoShape>, <String, NoShape>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:448, 15~23/        loss = F.depend(loss, self.optimizer(grads))/
  %2(CNode_67) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], network)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:443, 15~27/        loss = self.network(*inputs)/
  %3(CNode_68) = MakeTuple(%para0_inputs0, %para0_inputs1)
      : (<Tensor[Float32], (4, 3, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:441, 29~35/    def _no_sens_impl(self, *inputs):/

#------------------------> 9
  %4(loss) = DoUnpackCall(%2, %3)
      : (<Func, NoShape>, <Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:443, 15~36/        loss = self.network(*inputs)/
  %5(CNode_83) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], optimizer)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:448, 30~44/        loss = F.depend(loss, self.optimizer(grads))/
  %6(CNode_75) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], grad_reducer)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:445, 16~33/        grads = self.grad_reducer(grads)/
  %7(CNode_69) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], grad_no_sens)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:444, 16~33/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %8(CNode_70) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], network)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:444, 34~46/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %9(CNode_71) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<TrainOneStepCell::5218657648>'], weights)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:444, 48~60/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %10(CNode_74) = %7(%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:444, 16~61/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %11(grads) = DoUnpackCall(%10, %3)
      : (<null>, <Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:444, 16~70/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %12(grads) = %6(%11)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:445, 16~40/        grads = self.grad_reducer(grads)/
  %13(CNode_86) = %5(%12)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:448, 30~51/        loss = F.depend(loss, self.optimizer(grads))/
  %14(loss) = %1(%4, %13)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:448, 15~52/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%14)
      : (<null>)
      #scope: (Default)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:454, 8~19/        return loss/
}
# Order:
#   1: @2↓_no_sens_impl_39:CNode_91{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core: 1
subgraph instance: UnpackCall_40 : 0x122172218

subgraph @UnpackCall_40(%para0_Parameter_41, %para0_Parameter_42) {
  %0(CNode_92) = TupleGetItem(%para0_Parameter_42, I64(0))
      : (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 3, 360, 480)>)
      #scope: (Default)
  %1(CNode_93) = TupleGetItem(%para0_Parameter_42, I64(1))
      : (<Tuple[Tensor[Float32]*2], TupleShape((4, 3, 360, 480), (4, 1, 360, 480))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 1, 360, 480)>)
      #scope: (Default)

#------------------------> 10
  %2(CNode_94) = Parameter_41(%0, %1)
      : (<Tensor[Float32], (4, 3, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<null>)
      #scope: (Default)
  Return(%2)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @UnpackCall_40:CNode_94{[0]: param_Parameter_41, [1]: CNode_92, [2]: CNode_93}
#   2: @UnpackCall_40:CNode_95{[0]: ValueNode<Primitive> Return, [1]: CNode_94}


subgraph attr:
training: 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43 : 0x122175a18
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:123~125, 4~40/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43(%para0_data, %para0_label) {
  %0(CNode_96) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<WithLossCell::5219264176>'], _loss_fn)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:125, 15~28/        return self._loss_fn(out, label)/
  %1(CNode_97) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<WithLossCell::5219264176>'], _backbone)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:124, 14~28/        out = self._backbone(data)/
  %2(out) = %1(%para0_data)
      : (<Tensor[Float32], (4, 3, 360, 480)>) -> (<Tensor[Float32], (4, 11, 360, 480)>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:124, 14~34/        out = self._backbone(data)/

#------------------------> 11
  %3(CNode_98) = %0(%2, %para0_label)
      : (<Tensor[Float32], (4, 11, 360, 480)>, <Tensor[Float32], (4, 1, 360, 480)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:125, 15~40/        return self._loss_fn(out, label)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/wrap/cell_wrapper.py:125, 8~40/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:CNode_97{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<WithLossCell::5219264176>', [2]: ValueNode<Symbol> _backbone}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:CNode_99{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   4: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:out{[0]: CNode_97, [1]: param_data}
#   5: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:CNode_96{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.wrap.cell_wrapper..<WithLossCell::5219264176>', [2]: ValueNode<Symbol> _loss_fn}
#   6: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:CNode_100{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   8: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:CNode_98{[0]: CNode_96, [1]: out, [2]: param_label}
#   9: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_43:CNode_101{[0]: ValueNode<Primitive> Return, [1]: CNode_98}


subgraph attr:
training: 1
subgraph instance: __main___CustomSegLoss_construct_44 : 0x136a4fc18
# In file /Users/lmy/Documents/SegNet-Tutorial/train.py:22~31, 4~42/    def construct(self, logits, label):/
subgraph @__main___CustomSegLoss_construct_44(%para0_logits, %para0_label) {
  %0(CNode_102) = resolve(NameSpace[SymbolStr: 'Namespace:__main__'], print)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:24, 8~13/        print("[DEBUG] Before squeeze, label shape:", label.shape)/
  %1(CNode_103) = getattr(%para0_label, "shape")
      : (<Tensor[Float32], (4, 1, 360, 480)>, <String, NoShape>) -> (<Tuple[Int64*4], TupleShape(NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:24, 54~65/        print("[DEBUG] Before squeeze, label shape:", label.shape)/
  %2(CNode_104) = %0("[DEBUG] Before squeeze, label shape:", %1)
      : (<String, NoShape>, <Tuple[Int64*4], TupleShape(NoShape, NoShape, NoShape, NoShape)>) -> (<Tensor[Int32], (1)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:24, 8~66/        print("[DEBUG] Before squeeze, label shape:", label.shape)/
  %3(CNode_105) = resolve(NameSpace[SymbolStr: 'Namespace:__main__'], ops)
      : (<External, NoShape>, <External, NoShape>) -> (<External, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:26, 16~19/        label = ops.Squeeze(1)(label)/
  %4(CNode_106) = getattr(%3, "Squeeze")
      : (<External, NoShape>, <String, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:26, 16~27/        label = ops.Squeeze(1)(label)/
  %5(CNode_107) = %4(I64(1))
      : (<Int64, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:26, 16~30/        label = ops.Squeeze(1)(label)/
  %6(label) = %5(%para0_label)
      : (<Tensor[Float32], (4, 1, 360, 480)>) -> (<Tensor[Float32], (4, 360, 480)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:26, 16~37/        label = ops.Squeeze(1)(label)/
  %7(CNode_108) = getattr(%6, "shape")
      : (<Tensor[Float32], (4, 360, 480)>, <String, NoShape>) -> (<Tuple[Int64*3], TupleShape(NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:27, 53~64/        print("[DEBUG] After squeeze, label shape:", label.shape)/
  %8(CNode_109) = %0("[DEBUG] After squeeze, label shape:", %7)
      : (<String, NoShape>, <Tuple[Int64*3], TupleShape(NoShape, NoShape, NoShape)>) -> (<Tensor[Int32], (1)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:27, 8~65/        print("[DEBUG] After squeeze, label shape:", label.shape)/
  %9(CNode_110) = getattr(%para0_logits, "shape")
      : (<Tensor[Float32], (4, 11, 360, 480)>, <String, NoShape>) -> (<Tuple[Int64*4], TupleShape(NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:30, 39~51/        print("[DEBUG] Logits shape:", logits.shape, "Label shape:", label.shape)/
  %10(CNode_111) = getattr(%3, "Cast")
      : (<External, NoShape>, <String, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:29, 16~24/        label = ops.Cast()(label, mstype.int32)/
  %11(CNode_112) = %10()
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:29, 16~26/        label = ops.Cast()(label, mstype.int32)/
  %12(CNode_113) = resolve(NameSpace[SymbolStr: 'Namespace:__main__'], mstype)
      : (<External, NoShape>, <External, NoShape>) -> (<External, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:29, 34~40/        label = ops.Cast()(label, mstype.int32)/
  %13(CNode_114) = getattr(%12, "int32")
      : (<External, NoShape>, <String, NoShape>) -> (<TypeType, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:29, 34~46/        label = ops.Cast()(label, mstype.int32)/
  %14(label) = %11(%6, %13)
      : (<Tensor[Float32], (4, 360, 480)>, <TypeType, NoShape>) -> (<Tensor[Int32], (4, 360, 480)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:29, 16~47/        label = ops.Cast()(label, mstype.int32)/
  %15(CNode_115) = getattr(%14, "shape")
      : (<Tensor[Int32], (4, 360, 480)>, <String, NoShape>) -> (<Tuple[Int64*3], TupleShape(NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:30, 69~80/        print("[DEBUG] Logits shape:", logits.shape, "Label shape:", label.shape)/
  %16(CNode_116) = %0("[DEBUG] Logits shape:", %9, "Label shape:", %15)
      : (<String, NoShape>, <Tuple[Int64*4], TupleShape(NoShape, NoShape, NoShape, NoShape)>, <String, NoShape>, <Tuple[Int64*3], TupleShape(NoShape, NoShape, NoShape)>) -> (<Tensor[Int32], (1)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:30, 8~81/        print("[DEBUG] Logits shape:", logits.shape, "Label shape:", label.shape)/
  %17(CNode_117) = MakeTuple(%2, %8, %16)
      : (<Tensor[Int32], (1)>, <Tensor[Int32], (1)>, <Tensor[Int32], (1)>) -> (<Tuple[Tensor[Int32]*3], TupleShape((1), (1), (1))>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:22~31, 4~42/    def construct(self, logits, label):/
  %18(CNode_118) = StopGradient(%17)
      : (<Tuple[Tensor[Int32]*3], TupleShape((1), (1), (1))>) -> (<Tuple[Tensor[Int32]*3], TupleShape((1), (1), (1))>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:22~31, 4~42/    def construct(self, logits, label):/
  %19(CNode_119) = resolve(NameSpace[ClassMember: 'Namespace:__main__..<CustomSegLoss::5219012816>'], loss_fn)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:31, 15~27/        return self.loss_fn(logits, label)/

#------------------------> 12
  %20(CNode_120) = %19(%para0_logits, %14)
      : (<Tensor[Float32], (4, 11, 360, 480)>, <Tensor[Int32], (4, 360, 480)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:31, 15~42/        return self.loss_fn(logits, label)/
  %21(CNode_121) = Depend(%20, %18) primitive_attrs: {side_effect_propagate: I64(1)} cnode_attrs: {topo_sort_rhs_first: Bool(1)}
      : (<null>, <Tuple[Tensor[Int32]*3], TupleShape((1), (1), (1))>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:31, 15~42/        return self.loss_fn(logits, label)/
  Return(%21)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss)
      # In file /Users/lmy/Documents/SegNet-Tutorial/train.py:31, 8~42/        return self.loss_fn(logits, label)/
}
# Order:
#   1: @__main___CustomSegLoss_construct_44:CNode_102{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:__main__', [2]: ValueNode<Symbol> print}
#   2: @__main___CustomSegLoss_construct_44:CNode_103{[0]: ValueNode<Primitive> getattr, [1]: param_label, [2]: ValueNode<StringImm> shape}
#   3: @__main___CustomSegLoss_construct_44:CNode_122{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   5: @__main___CustomSegLoss_construct_44:CNode_104{[0]: CNode_102, [1]: ValueNode<StringImm> [DEBUG] Before squeeze, label shape:, [2]: CNode_103}
#   6: @__main___CustomSegLoss_construct_44:CNode_105{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:__main__', [2]: ValueNode<Symbol> ops}
#   7: @__main___CustomSegLoss_construct_44:CNode_106{[0]: ValueNode<Primitive> getattr, [1]: CNode_105, [2]: ValueNode<StringImm> Squeeze}
#   8: @__main___CustomSegLoss_construct_44:CNode_123{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  10: @__main___CustomSegLoss_construct_44:CNode_107{[0]: CNode_106, [1]: ValueNode<Int64Imm> 1}
#  11: @__main___CustomSegLoss_construct_44:CNode_124{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  13: @__main___CustomSegLoss_construct_44:label{[0]: CNode_107, [1]: param_label}
#  14: @__main___CustomSegLoss_construct_44:CNode_108{[0]: ValueNode<Primitive> getattr, [1]: label, [2]: ValueNode<StringImm> shape}
#  15: @__main___CustomSegLoss_construct_44:CNode_125{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  17: @__main___CustomSegLoss_construct_44:CNode_109{[0]: CNode_102, [1]: ValueNode<StringImm> [DEBUG] After squeeze, label shape:, [2]: CNode_108}
#  18: @__main___CustomSegLoss_construct_44:CNode_111{[0]: ValueNode<Primitive> getattr, [1]: CNode_105, [2]: ValueNode<StringImm> Cast}
#  19: @__main___CustomSegLoss_construct_44:CNode_112{[0]: CNode_111}
#  20: @__main___CustomSegLoss_construct_44:CNode_113{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:__main__', [2]: ValueNode<Symbol> mstype}
#  21: @__main___CustomSegLoss_construct_44:CNode_114{[0]: ValueNode<Primitive> getattr, [1]: CNode_113, [2]: ValueNode<StringImm> int32}
#  22: @__main___CustomSegLoss_construct_44:CNode_126{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  24: @__main___CustomSegLoss_construct_44:label{[0]: CNode_112, [1]: label, [2]: CNode_114}
#  25: @__main___CustomSegLoss_construct_44:CNode_110{[0]: ValueNode<Primitive> getattr, [1]: param_logits, [2]: ValueNode<StringImm> shape}
#  26: @__main___CustomSegLoss_construct_44:CNode_115{[0]: ValueNode<Primitive> getattr, [1]: label, [2]: ValueNode<StringImm> shape}
#  27: @__main___CustomSegLoss_construct_44:CNode_127{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  29: @__main___CustomSegLoss_construct_44:CNode_116{[0]: CNode_102, [1]: ValueNode<StringImm> [DEBUG] Logits shape:, [2]: CNode_110, [3]: ValueNode<StringImm> Label shape:, [4]: CNode_115}
#  30: @__main___CustomSegLoss_construct_44:CNode_119{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:__main__..<CustomSegLoss::5219012816>', [2]: ValueNode<Symbol> loss_fn}
#  31: @__main___CustomSegLoss_construct_44:CNode_128{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  33: @__main___CustomSegLoss_construct_44:CNode_120{[0]: CNode_119, [1]: param_logits, [2]: label}
#  35: @__main___CustomSegLoss_construct_44:CNode_129{[0]: ValueNode<Primitive> Return, [1]: CNode_121}
#  36: @__main___CustomSegLoss_construct_44:CNode_114{[0]: ValueNode<PrimitivePy> DtypeToEnum, [1]: ValueNode<StringImm> Cast, [2]: ValueNode<StringImm> dtype, [3]: CNode_114}


subgraph attr:
training: 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45 : 0x1221fde18
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:855~864, 4~31/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45(%para0_logits, %para0_labels) {
  %0(CNode_130) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.nn.loss.loss'], _check_is_tensor)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:856, 8~24/        _check_is_tensor('logits', logits, self.cls_name)/
  %1(CNode_131) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.loss.loss..<SoftmaxCrossEntropyWithLogits::5218822288>'], cls_name)
      : (<External, NoShape>, <External, NoShape>) -> (<String, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:856, 43~56/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_132) = %0("logits", %para0_logits, %1)
      : (<String, NoShape>, <Tensor[Float32], (4, 11, 360, 480)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:856, 8~57/        _check_is_tensor('logits', logits, self.cls_name)/
  %3(CNode_133) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.loss.loss..<SoftmaxCrossEntropyWithLogits::5218822288>'], cls_name)
      : (<External, NoShape>, <External, NoShape>) -> (<String, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:857, 43~56/        _check_is_tensor('labels', labels, self.cls_name)/
  %4(CNode_134) = %0("labels", %para0_labels, %3)
      : (<String, NoShape>, <Tensor[Int32], (4, 360, 480)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:857, 8~57/        _check_is_tensor('labels', labels, self.cls_name)/
  %5(CNode_135) = MakeTuple(%2, %4)
      : (<None, NoShape>, <None, NoShape>) -> (<Tuple[None*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:855~864, 4~31/    def construct(self, logits, labels):/
  %6(CNode_136) = StopGradient(%5)
      : (<Tuple[None*2], TupleShape(NoShape, NoShape)>) -> (<Tuple[None*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:855~864, 4~31/    def construct(self, logits, labels):/

#------------------------> 13
  %7(CNode_137) = call @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_46()
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:858~862, 8~93/        if self.sparse:/
  %8(CNode_138) = Depend(%7, %6) primitive_attrs: {side_effect_propagate: I64(1)} cnode_attrs: {topo_sort_rhs_first: Bool(1)}
      : (<null>, <Tuple[None*2], TupleShape(NoShape, NoShape)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:858~862, 8~93/        if self.sparse:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:858~862, 8~93/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_130{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.nn.loss.loss', [2]: ValueNode<Symbol> _check_is_tensor}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_131{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.loss.loss..<SoftmaxCrossEntropyWithLogits::5218822288>', [2]: ValueNode<Symbol> cls_name}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_139{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_132{[0]: CNode_130, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: CNode_131}
#   6: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_133{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.loss.loss..<SoftmaxCrossEntropyWithLogits::5218822288>', [2]: ValueNode<Symbol> cls_name}
#   7: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_140{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   9: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_134{[0]: CNode_130, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: CNode_133}
#  10: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_137{[0]: ValueNode<FuncGraph> ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_46}
#  12: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:CNode_141{[0]: ValueNode<Primitive> Return, [1]: CNode_138}


subgraph attr:
training: 1
subgraph instance: ✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_46 : 0x122183a18
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:855~864, 4~31/    def construct(self, logits, labels):/
subgraph @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_46 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45]() {

#------------------------> 14
  %0(CNode_142) = call @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47()
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:859~861, 12~24/            if self.reduction == 'mean':/
  Return(%0)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:859~861, 12~24/            if self.reduction == 'mean':/
}
# Order:
#   1: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_46:CNode_142{[0]: ValueNode<FuncGraph> 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47}
#   2: @✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_46:CNode_143{[0]: ValueNode<Primitive> Return, [1]: CNode_142}


subgraph attr:
training: 1
subgraph instance: 2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47 : 0x1221fe418
# In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:855~864, 4~31/    def construct(self, logits, labels):/
subgraph @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45]() {
  %0(CNode_144) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.loss.loss..<SoftmaxCrossEntropyWithLogits::5218822288>'], sparse_softmax_cross_entropy)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:860, 20~53/                x = self.sparse_softmax_cross_entropy(logits, labels)/

#------------------------> 15
  %1(x) = %0($(@mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:para0_logits), $(@mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_45:para0_labels))
      : (<Tensor[Float32], (4, 11, 360, 480)>, <Tensor[Int32], (4, 360, 480)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:860, 20~69/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-CustomSegLoss/loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file /opt/anaconda3/envs/mindspore/lib/python3.10/site-packages/mindspore/nn/loss/loss.py:861, 16~24/                return x/
}
# Order:
#   1: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47:CNode_144{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.loss.loss..<SoftmaxCrossEntropyWithLogits::5218822288>', [2]: ValueNode<Symbol> sparse_softmax_cross_entropy}
#   2: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47:CNode_145{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   4: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47:x{[0]: CNode_144, [1]: param_logits, [2]: param_labels}
#   5: @2✓mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_47:CNode_146{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 16/20 (Ignored 4 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
No more function graphs.

